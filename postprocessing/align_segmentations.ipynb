{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split up test results into tp_gt, tp_pred, fp_pred, fn_gt\n",
    "\n",
    "We are interested in ways in which the network messes up. One way to investigate this is by checking what the raw data looks like when the network makes a mistake. It is also of interest to see what the raw data looks like when the network does well.\n",
    "\n",
    "To do this we need to align the raw data from each segmented object. We do this by taking the segmentations (either ground truth or predicted) and fitting a line to their voxels. Once we have the lines for each object, we can determine the rotation matrix to align them all to a single line, e.g. (0,0,1). We can then apply that rotation matrix to each objects corresponding raw data. \n",
    "\n",
    "The alignment step requires that - for each object - we have its center of mass, and bounding box. The rest of this code assumes you have your data split up into the following datasets: tp_gt, tp_pred, fp_pred, fn_gt and that you have the corresponding bounding box and center of mass information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Lines to Segmentations\n",
    "Here we fit lines to the predicted segmentations. To do so we first must run some analysis code to find the bounding box of each corresponding to true positive, false positive etc. This is in a separate repo but could also be done here. We then look at each segmentated object one at a time and fit a line to its voxels. We then project all these voxels along this line to find the extents of the line to make it a line segment. We then write this out as neuroglancer annotations. You can also write these out however you would like, you would just have to update the downstream code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from neuroglancer.write_annotations import AnnotationWriter\n",
    "from neuroglancer import AnnotationPropertySpec\n",
    "from neuroglancer.coordinate_space import CoordinateSpace\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from funlib.geometry import Roi\n",
    "from funlib.persistence import open_ds\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def find_min_max_projected_points(points, line_point, line_direction):\n",
    "    # chatgpt\n",
    "    line_direction = line_direction / np.linalg.norm(\n",
    "        line_direction\n",
    "    )  # Normalize direction vector\n",
    "\n",
    "    # Calculate the vector from line_point to each point\n",
    "    point_vectors = points - line_point\n",
    "\n",
    "    # Calculate the projection scalar for each point using dot product and broadcasting\n",
    "    projection_scalars = np.sum(point_vectors * line_direction, axis=1)\n",
    "\n",
    "    # Calculate the projected points for each point\n",
    "    projected_points = line_point + projection_scalars[:, np.newaxis] * line_direction\n",
    "\n",
    "    # Find the minimum and maximum projection scalar indices\n",
    "    min_projection_idx = np.argmin(projection_scalars)\n",
    "    max_projection_idx = np.argmax(projection_scalars)\n",
    "\n",
    "    return projected_points[min_projection_idx], projected_points[max_projection_idx]\n",
    "\n",
    "\n",
    "dataset = \"jrc_22ak351-leaf-3m\"\n",
    "run = \"finetuned_3d_lsdaffs_weight_ratio_0.50_plasmodesmata_pseudorandom_training_centers_maxshift_18_more_annotations_unet_default_v2_no_dataset_predictor_node_lr_5E-5__1\"\n",
    "iteration = \"140000\"\n",
    "yaml_name = f\"{dataset}_2023-12-06\"\n",
    "\n",
    "# since it is arbitrary to have endpoints for line segment in terms of fitting, will just fit a line and then truncate it\n",
    "for ds_name in [\"tp_gt\", \"tp_pred\", \"fn_gt\", \"fp_pred\"]:\n",
    "    annotation_writer = AnnotationWriter(\n",
    "        CoordinateSpace(names=(\"x\", \"y\", \"z\"), scales=(1, 1, 1), units=\"nm\"),\n",
    "        annotation_type=\"line\",\n",
    "        properties=[\n",
    "            AnnotationPropertySpec(id=\"identifier\", type=\"uint16\"),\n",
    "        ],\n",
    "    )\n",
    "    df = pd.read_csv(\n",
    "        f\"/nrs/cellmap/ackermand/forAnnotators/leaf-gall/analysisResults/{yaml_name}/{ds_name}.csv\"\n",
    "    )\n",
    "    ds = open_ds(\n",
    "        f\"/nrs/cellmap/ackermand/forAnnotators/leaf-gall/{yaml_name}.n5\", ds_name\n",
    "    )\n",
    "    # uu, dd, vv = np.linalg.svd(data - datamean, full_matrices=False)\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # object id, bounding box and center of mass information, calculated beforehand\n",
    "        id = row[\"Object ID\"]\n",
    "        cube_min = np.array([row[f\"MIN {d} (nm)\"] for d in [\"Z\", \"Y\", \"X\"]])\n",
    "        cube_max = np.array([row[f\"MAX {d} (nm)\"] for d in [\"Z\", \"Y\", \"X\"]])\n",
    "        com = np.array([row[f\"COM {d} (nm)\"] for d in [\"Z\", \"Y\", \"X\"]])\n",
    "\n",
    "        # define an roi to actually ecompass the bounding box\n",
    "        roi = Roi(cube_min - 8, (cube_max - cube_min) + 16)\n",
    "\n",
    "        # only look at pixels corresponding to current object\n",
    "        data = np.column_stack(np.where(ds.to_ndarray(roi) == id))\n",
    "\n",
    "        # fit line to object voxels\n",
    "        uu, dd, vv = np.linalg.svd(data - np.mean(data, axis=0), full_matrices=False)\n",
    "        line_direction = vv[0]\n",
    "        line_origin = com\n",
    "\n",
    "        # find endpoints of line segment so that we can write it as neuroglancer annotations\n",
    "        start_point, end_point = find_min_max_projected_points(\n",
    "            data * 8 + 4 + roi.begin, line_origin, line_direction\n",
    "        )\n",
    "\n",
    "        # write out lines as neuroglancer annotations\n",
    "        annotation_writer.add_line(\n",
    "            point_a=start_point[::-1],\n",
    "            point_b=end_point[::-1],\n",
    "            id=int(id),\n",
    "            identifier=int(id),\n",
    "        )\n",
    "\n",
    "    annotation_writer.write(\n",
    "        f\"/groups/cellmap/cellmap/ackermand/neuroglancer_annotations/leaf-gall/forAnnotators/{yaml_name}/{ds_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align Segmentations\n",
    "Here we read in the fitted line annotations and calculate the rotation matrix to align the fitted lines to [0,0,1]. We take those rotation matrices and the resepective raw data corresponding to each fitted line and align rotate/align the raw data and write it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.diagnostics\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "import numpy as np\n",
    "from funlib.persistence import open_ds\n",
    "from funlib.geometry import Roi\n",
    "import numpy as np\n",
    "from funlib.geometry import Roi\n",
    "import scipy\n",
    "from funlib.persistence import open_ds\n",
    "from utils.utils import get_rotation_matrix, extract_precomputed_annotations\n",
    "from funlib.persistence import prepare_ds\n",
    "from funlib.geometry import Coordinate, Roi\n",
    "\n",
    "# Info to get paths\n",
    "dataset = \"jrc_22ak351-leaf-3m\"\n",
    "yaml_name = f\"{dataset}_2023-12-06\"\n",
    "\n",
    "ds = open_ds(\n",
    "    f\"/nrs/cellmap/data/{dataset}/{dataset}.n5\",\n",
    "    \"/em/fibsem-uint8/s0\",\n",
    ")\n",
    "\n",
    "\n",
    "pad = 60  # want this to be the cube size, but we need to make sure that all rotated cubes contain voxels in this region even when rotated, so we need this larger cubes\n",
    "extra_pad = int(np.ceil(np.sqrt(3) * pad))\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def read_and_align_image(current_pd):\n",
    "    pd_start = current_pd[:3]\n",
    "    pd_end = current_pd[3:]\n",
    "    pd_center = np.round(0.5 * (pd_start + pd_end) / 8) * 8\n",
    "    # https://math.stackexchange.com/questions/180418/calculate-rotation-matrix-to-align-vector-a-to-vector-b-in-3d/476311#476311\n",
    "    v1 = pd_start - pd_end\n",
    "    # v1 = np.array([1, 0, 0])\n",
    "    v2 = np.array([0, 0, 1])\n",
    "    rot = get_rotation_matrix(v1, v2)\n",
    "\n",
    "    roi = Roi((pd_center - extra_pad * 8)[::-1], [(extra_pad * 2 + 1) * 8] * 3)\n",
    "    im = ds.to_ndarray(roi).swapaxes(0, 2)\n",
    "    # Translation matrix to shift the image center to the origin\n",
    "    x, y, z = im.shape\n",
    "    trans = np.array((-x / 2, -y / 2, -z / 2))\n",
    "\n",
    "    T = np.identity(4)\n",
    "    T[:3, -1] = trans\n",
    "    im = scipy.ndimage.affine_transform(\n",
    "        im, np.linalg.inv(np.linalg.inv(T).dot(rot).dot(T)), order=0\n",
    "    )\n",
    "    im = im[\n",
    "        extra_pad - pad : extra_pad + pad + 1,\n",
    "        extra_pad - pad : extra_pad + pad + 1,\n",
    "        extra_pad - pad : extra_pad + pad + 1,\n",
    "    ]\n",
    "    return da.from_array(im)\n",
    "\n",
    "\n",
    "# Loop over the datasets\n",
    "for ds_name in [\"tp_gt\", \"tp_pred\", \"fn_gt\", \"fp_pred\"]:\n",
    "    with Client(threads_per_worker=1, n_workers=1) as client:\n",
    "        client.cluster.scale(10)\n",
    "        print(client.dashboard_link.replace(\"127.0.0.1\", \"ackermand-ws2.hhmi.org\"))\n",
    "\n",
    "        # Extract neuroglancer annotations as numpy array\n",
    "        _, pds = extract_precomputed_annotations(\n",
    "            f\"/groups/cellmap/cellmap/ackermand/neuroglancer_annotations/leaf-gall/forAnnotators/{yaml_name}/{ds_name}\"\n",
    "        )\n",
    "\n",
    "        # Align the images\n",
    "        aligned_images = [\n",
    "            da.from_delayed(\n",
    "                read_and_align_image(current_pd),\n",
    "                (extra_pad * 2 + 1, extra_pad * 2 + 1, extra_pad * 2 + 1),\n",
    "                np.float64,\n",
    "            )\n",
    "            for current_pd in pds\n",
    "        ]\n",
    "        # Stack all small Dask arrays into one\n",
    "        stack = da.stack(aligned_images, axis=0)\n",
    "        composite_image = stack.mean(axis=0).compute()\n",
    "\n",
    "        roi = Roi((0, 0, 0), np.array(composite_image.shape) * 8)\n",
    "        output_ds = prepare_ds(\n",
    "            f\"/nrs/cellmap/ackermand/forAnnotators/leaf-gall/{yaml_name}.n5\",\n",
    "            f\"composite_images/{ds_name}\",\n",
    "            total_roi=roi,\n",
    "            voxel_size=np.array([8, 8, 8]),\n",
    "            dtype=np.uint8,\n",
    "            write_size=Coordinate(3 * [8 * 128]),\n",
    "        )\n",
    "        output_ds[roi] = composite_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellmap_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
